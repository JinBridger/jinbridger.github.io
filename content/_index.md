---
title: 主页
type: docs
bookToc: false
---

![Ling, Cantabile and Mulberry](https://i.imgur.com/jpS2YpR.png)

{{< hint info >}}
**欢迎来到我的博客！🥰**
{{< /hint >}}

{{< hint warning >}}
**我目前在 Intel 实习，正在参与一些有趣的项目**

**[💫 Intel® LLM library for PyTorch](https://github.com/intel-analytics/ipex-llm)** - Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, Baichuan, Mixtral, Gemma, Phi, MiniCPM, etc.) on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max); seamlessly integrate with llama.cpp, Ollama, HuggingFace, LangChain, LlamaIndex, GraphRAG, DeepSpeed, vLLM, FastChat, Axolotl, etc. 

{{< /hint >}}
