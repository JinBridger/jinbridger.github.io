---
title: ä¸»é¡µ
type: docs
bookToc: false
date: 2024-10-30
---

![Ling, Cantabile and Mulberry](https://i.imgur.com/jpS2YpR.png)

{{< hint info >}}
**æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ï¼ğŸ¥°**

[![GitHub](https://img.shields.io/badge/GitHub-JinBridger-cornflowerblue.svg)](https://github.com/JinBridger)
[![Mail](https://img.shields.io/badge/Mail-jinqiao@seu.edu.cn-steelblue.svg)](mailto:jinqiao@seu.edu.cn)
{{< /hint >}}

{{< hint warning >}}
**æˆ‘ç›®å‰åœ¨ Intel å®ä¹ ï¼Œæ­£åœ¨å‚ä¸ä¸€äº›æœ‰è¶£çš„é¡¹ç›®**

**[ğŸ’« IntelÂ® LLM library for PyTorch](https://github.com/intel-analytics/ipex-llm)** - Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, Baichuan, Mixtral, Gemma, Phi, MiniCPM, etc.) on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max); seamlessly integrate with llama.cpp, Ollama, HuggingFace, LangChain, LlamaIndex, GraphRAG, DeepSpeed, vLLM, FastChat, Axolotl, etc. 

{{< /hint >}}
