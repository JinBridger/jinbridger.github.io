---
title: 主页
type: docs
bookToc: false
---

![Ling and Chongyue](/image/main-page/ling-and-chongyue.png)

{{< hint info >}}
**欢迎来到我的博客！🥰**
{{< /hint >}}

{{< hint warning >}}
**我目前在 Intel 实习，正在参与一些有趣的项目：**

>**[💫 IPEX-LLM](https://github.com/intel-analytics/ipex-llm)** - Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, Baichuan, Mixtral, Gemma, etc.) on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max). A PyTorch LLM library that seamlessly integrates with llama.cpp, Ollama, HuggingFace, LangChain, LlamaIndex, DeepSpeed, vLLM, FastChat, etc. 

{{< /hint >}}
