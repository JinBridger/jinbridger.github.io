---
title: "信息论入门笔记"
weight: 1
date: 2025-09-07
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

# 信息论入门笔记

作为人工智能的基石, 信息论提供了从数学上量化信息的方式, 指导着人工智能的诸多方面, 例如损失函数的设计等.

## 自信息量

信息论里一个重要的概念是信息. 香农从人类认识的角度出发, 认为信息是 认识主体接收到的 可以消除对事物认识不确定性的新内容.
在认识事物的过程中, 需要发生关于这个事物的事件, 然后通过发生的事件来获取对这个事物的信息.

那怎么衡量这个事件带来的信息有多少呢, 香农认为这与事件的发生概率有关. 假设事件 $x$ 发生的概率是 $p(x)$, 那么
- $p(x)$ 越大, $x$ 所能带来的信息就越小.
- $p(x)$ 越小, $x$ 所能带来的信息就越大.

这里说的信息, 就是指这个事件的**自信息量**. 换句话说, 可以把自信息量理解为惊讶程度.
用 $I(x)$ 来表示事件 $x$ 的自信息量.

从人类认识的角度出发, 自信息量 $I(x)$ 应该有以下的性质:
1. 如果 $p(x_i) > p(x_j)$, 那么 $I(x_i) < I(x_j)$
2. 如果 $p(x) = 0$, 那么 $I(x)=\inf$
3. 如果 $p(x) = 1$, 那么 $I(x) = 0$
4. 对于两个独立的事件 $x$ 与 $y$, $I(x, y) = I(x) + I(y)$

可以证明满足这四条性质的函数就是负的对数函数, 因此定义 $$I(x) = -\log p(x)$$

## 信息熵

在有了自信息量这一概念以后, 就可以引入信息熵. 信息熵可以理解为自信息量的期望.

假设有一个随机变量 $X$, 其可能的取值包括 $[x_1, x_2, ..., x_n]$, 根据自信息量的定义, $X=x_i$ 这一事件的信息就是 $I(x_i)=-\log p(x_i)$. 对于事件的信息可以用自信息量来定义, 那么对于这个分布, 如何定义其携带的信息有多少呢? 这里就引入了信息熵这一概念.

一个直觉的想法是用自信息量的期望来表示一个分布所携带的信息量, 这就是信息熵. 对于随机变量 $X$, 其信息熵用 $H(X)$ 表示, 定义为
$$H(X) = E(I(x)) = -\sum_{i=1}^n p(x_i)\log p(x_i)$$

换个角度来说, 假设我们去观察 $X$ 这个随机变量, 那么随着观测次数的增加, 我们平均每次观测获得的信息量就收敛于 $X$ 的信息熵.

当 $X$ 服从均匀分布的时候, 信息熵 $H(X)$ 取最大值 $\log K$. 可以这样理解: 均匀分布由于各个事件发生的概率是一样的, 因此非常难预测. 所以这个分布所携带的信息是最多的.

从另一种角度来说, 信息熵也可以理解为一个分布的不确定性有多大.

## 交叉熵

假设有一个随机变量 $X$ 服从分布 $P(X)$. 而我们并不知道 $P(X)$ 是多少. 我们猜了一个可能的分布 $Q(X)$. 

假设我们用猜的分布 $Q(X)$ 去观测 $X$, 那么随着观测次数增加, 观测到的 $X$ 的信息熵收敛于
$$H(P, Q) = -\sum_{i=1}^n p(x_i)\log q(x_i)$$

也就是交叉熵. 可以理解为, 交叉熵就是我们用自己猜的分布去观测 $X$ 得到的「信息熵」

## 相对熵

怎么来量化这个分布相对与真正的分布相差了多少呢? 这里就引入了相对熵的概念.

由于我们猜的分布 $Q(X)$ 并不是实际的分布 $P(X)$, 因此我们实际上高估了 $X$ 的信息熵 (这里一定是高估了). $X$ 的真正的信息熵是
$$H(X) = -\sum_{i=1}^n p(x_i)\log p(x_i)$$

将交叉熵与真正的信息熵相减即可得到我们高估了多少. 高估的部分就是相对熵. 用 $D_\text{KL}(P\parallel Q)$ 来表示

$$D_\text{KL}(P\parallel Q) = H(P, Q) - H(X) = -\sum_{i=1}^n p(x_i)\log \frac{q(x_i)}{p(x_i)}$$

相对熵也叫 KL 散度. 从另一个角度看, 相对熵也可以理解为我们猜的分布 $Q(X)$ 距离真正的分布 $P(X)$ 有多远. 相对熵越大, 说明我们猜的分布 $Q(X)$ 距离真的分布 $P(X)$ 越远.


## 互信息

假设我们有两个随机变量 $X$, $Y$. **这两个随机变量不相互独立.**
那么, 从信息论的角度出发, 它们必然有信息上的重叠.
互信息要做的就是表示两个随机变量的信息重叠了多少.

我们可以用文氏图来表示两个不独立的随机变量的信息熵. 图中面积代表信息熵的大小, 蓝色椭圆的面积代表 $X$ 的信息熵, 也就是 $H(X)$, 同理, 红色椭圆的面积代表 $H(Y)$. 由于两个变量不独立, 因此两个椭圆必然有重叠. 重叠的面积就是 $X$ 与 $Y$ 重叠的信息量, 也就是互信息, 用 $I(X;Y)$ 表示:

<div align="center">
	<img id="entropy_auto_svg" src="/image/mlai/information-theory-basic/entropy.svg" width="50%">
    <br>
    <div style="display: inline-block; color: #999; padding: 2px;">
    用 Venn 图表示两个不独立的随机变量的信息熵.
    </div>
</div>

从图中可以看到, 要求椭圆重叠部分的面积, 我们可以用将两个椭圆相加, 再减去两个椭圆总共的面积, 即
$$I(X;Y)=H(X) + H(Y) - H(X, Y)$$

我们先计算这两个随机变量总共有多少信息, 根据信息熵公式有:
$$H(X, Y) = -\sum_{i=1}^n \sum_{j=1}^m p(x_i, y_i)\log p(x_i, y_i)$$

我们再计算 $X$ 与 $Y$ 各自有多少信息:
$$H(X) = -\sum_{i=1}^n p(x_i)\log p(x_i)$$
$$H(Y) = -\sum_{j=1}^m p(y_j)\log p(y_j)$$

代入公式可得
$$
\begin{aligned}
I(X;Y)&=H(X) + H(Y) - H(X, Y)\\\\
&= - H(X, Y)+H(X) + H(Y)\\\\
&= \sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j)\log p(x_i, y_j) -\sum_{i=1}^n p(x_i)\log p(x_i) -\sum_{j=1}^m p(y_j)\log p(y_j)\\\\
&= \sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j)\log p(x_i, y_j) -\sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j)\log p(x_i) -\sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j)\log p(y_j)\\\\
&= \sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j)(\log p(x_i, y_j) - \log p(x_i) - \log p_i(y_j))\\\\
&= \sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j)\log \frac{p(x_i, y_j)}{p(x_i)p(y_j)} 
\end{aligned}
$$

根据条件概率公式
$$p(y_j | x_i) = \frac{p(x_i, y_j)}{p(x_i)}$$

可以将互信息写为
$$
I(X;Y)=\sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j)\log \frac{p(y_j|x_i)}{p(y_j)}
$$
